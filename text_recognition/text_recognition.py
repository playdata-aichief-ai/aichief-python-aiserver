import string
import argparse

import os
import torch
import torch.backends.cudnn as cudnn
import torch.utils.data
import torch.nn.functional as F

from utils.logging_time import logging_time

from .utils import CTCLabelConverter, AttnLabelConverter
from .dataset import RawDataset, AlignCollate
from .model import Model
from ai.settings.settings import BASE_DIR

"""
CUDA_VISIBLE_DEVICES=0 python3 demo.py \
--Transformation TPS --FeatureExtraction ResNet --SequenceModeling BiLSTM --Prediction Attn \
--image_folder demo_image/ \
--saved_model saved_models/TPS-ResNet-BiLSTM-Attn-Seed1111/best_accuracy.pth
"""


class Text_Recognition():

    def __init__(self, Transformation='TPS', FeatureExtraction='ResNet', SequenceModeling='BiLSTM', Prediction='Attn', saved_model=os.path.join(BASE_DIR, 'text_recognition', 'saved_models', 'best_accuracy_lenplus_66.pth')):
        parser = argparse.ArgumentParser()
        parser.add_argument('--image_folder',   # required=True,
                            help='path to image_folder which contains text images')
        parser.add_argument('--workers', type=int,
                            help='number of data loading workers', default=4)
        parser.add_argument('--batch_size', type=int, default=500,  # trainingì€ ì•ˆë˜ëŠ”ë°, predictëŠ” ê°€ëŠ¥. trainingë•Œ 32í—€ìŒ
                            help='input batch size')  # 192
        parser.add_argument('--saved_model', default=saved_model,  # required=True,
                            help="path to saved_model to evaluation")
        """ Data processing """
        parser.add_argument('--batch_max_length', type=int,
                            default=25, help='maximum-label-length')
        parser.add_argument('--imgH', type=int, default=32,
                            help='the height of the input image')
        parser.add_argument('--imgW', type=int, default=100,
                            help='the width of the input image')
        parser.add_argument('--rgb', action='store_true', help='use rgb input')
        # parser.add_argument('--character', type=str, default='!"#$%&\'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\]^_`abcdefghijklmnopqrstuvwxyz{|}~Â¨ê°€ê°ê°„ê°ˆê°ê°‘ê°’ê°“ê°”ê°•ê°–ê°—ê°™ê°œê°ê°¤ê°¬ê°­ê°±ê±€ê±ê±°ê±±ê±´ê±·ê±¸ê²€ê²ê²ƒê²…ê²‰ê²Œê²ê²ê²”ê²Ÿê² ê²¨ê²©ê²ªê²¬ê²°ê²³ê²¸ê²¹ê²¼ê²½ê³ê³„ê³ ê³¡ê³¤ê³§ê³¨ê³ªê³°ê³±ê³³ê³µê³·ê³¼ê³½ê´€ê´„ê´‘ê´œê´´êµêµ¬êµ­êµ°êµ³êµ´êµµêµ¼êµ½êµ¿ê¶ê¶ˆê¶Œê¶¤ê·€ê·„ê·œê·ê· ê·¤ê·¬ê·­ê·¸ê·¹ê·¼ê·¿ê¸€ê¸ê¸ˆê¸‰ê¸‹ê¸ê¸°ê¸±ê¸´ê¸¸ê¹€ê¹ê¹…ê¹Šê¹Œê¹ê¹ê¹ê¹”ê¹ê¹¥ê¹¨êº êº¼êº¾ê»„ê»Œê»ê»‘ê»˜ê»´ê¼ˆê¼‰ê¼ê¼¬ê¼­ê¼¼ê¼½ê½‚ê½ƒê½ˆê½‰ê¾¸ê¾¹ê¿€ê¿ˆê¿ê¿ê¿”ê¿°ë€Œë„ëˆëŠëŒë“ë”ë—ëë¼ë‚€ë‚„ë‚Œë‚˜ë‚™ë‚œë‚ ë‚¨ë‚©ë‚«ë‚¬ë‚­ë‚®ë‚¯ë‚±ë‚´ë‚µë‚¸ë‚¼ëƒ„ëƒ…ëƒˆëƒ‰ëƒ ëƒ¥ë„ˆë„‰ë„Œë„ë„“ë„˜ë„™ë„›ë„£ë„¤ë„¥ë„¨ë„¬ë„´ë„µë„·ë…€ë…ë…„ë…ˆë…ë…•ë…›ë…¸ë…¹ë…¼ë†€ë†ˆë†ë†’ë†“ë†”ë‡Œë‡¨ëˆ„ëˆ…ëˆˆëˆŒëˆ”ëˆ•ëˆ—ëˆ™ë‰˜ë‰´ë‰¸ë‰¼ëŠ„ëŠ…ëŠëŠ”ëŠ˜ëŠ¥ëŠ¦ëŠ¬ë‹ˆë‹‰ë‹Œë‹ë‹ë‹˜ë‹™ë‹›ë‹ë‹¡ë‹¤ë‹¥ë‹¦ë‹¨ë‹«ë‹¬ë‹­ë‹®ë‹´ë‹µë‹·ë‹¹ë‹»ë‹½ë‹¿ëŒ€ëŒ„ëŒˆëŒ•ëŒœë”ë•ë˜ëœë¤ë¥ë§ë©ë®ë°ë±ë´ë¸ë€ëëŒë°ë„ë…ëˆë‹ëŒëë”ë•ë—ë™ëë ë¼ë˜ëœë ë¨ë©ë«ë‘ë‘‘ë‘”ë‘˜ë‘ ë‘¡ë‘£ë‘¥ë‘ªë’¤ë’·ë“€ë“ˆë“ë“œë“ë“ ë“£ë“¤ë“¬ë“­ë“¯ë“±ë“¸ë””ë”•ë”˜ë”œë”¤ë”¥ë”©ë”ªë”°ë”±ë”¸ë•€ë•…ë•Œë•ë•¡ë•¨ë–„ë– ë–¡ë–¤ë–¨ë–»ë–¼ë–½ë—„ë—´ë˜ë˜‘ë˜˜ë˜¥ë™¤ëšœëšëš¦ëš«ëš¯ë›°ëœ¨ëœ©ëœ¬ëœ¯ëœ°ëœ¸ëœ»ë„ë ë¤ë¼ë½ë€ë„ë†ëŒëëë‘ë—ë˜ë™ëœë ë¨ë©ë«ë­ëµëŸ‰ëŸ¬ëŸ­ëŸ°ëŸ´ëŸ¼ëŸ½ëŸ¿ë €ë ‡ë ˆë ‰ë Œë ë ˜ë ™ë ›ë ë ¤ë ¥ë ¨ë ¬ë ´ë µë ·ë ¸ë ¹ë¡€ë¡œë¡ë¡ ë¡¤ë¡¬ë¡­ë¡¯ë¡±ë¡²ë¡¼ë¢°ë¢´ë£Œë£”ë£¡ë£¨ë£©ë£¬ë£°ë£¸ë£¹ë£»ë£½ë¤€ë¤¼ë¥˜ë¥ ë¥¨ë¥©ë¥´ë¥µë¥¸ë¥¼ë¦„ë¦…ë¦‰ë¦ë¦ë¦”ë¦¬ë¦­ë¦°ë¦´ë¦¼ë¦½ë¦¿ë§ë§„ë§‡ë§ˆë§‰ë§Œë§ë§ë§‘ë§˜ë§›ë§ë§ë§Ÿë§¡ë§¤ë§¥ë§¨ë§´ë§·ë§¹ë§»ë¨€ë¨ë¨œë¨¸ë¨¹ë¨¼ë©€ë©ë©ˆë©‹ë©ë©ë©”ë©•ë©˜ë©œë©°ë©´ë©¸ëª…ëª‡ëª”ëª¨ëª©ëª¬ëª°ëª¸ëª¹ëª»ëª½ë«¼ë¬˜ë¬´ë¬µë¬¸ë¬ºë¬»ë¬¼ë¬½ë¬¿ë­‚ë­‡ë­‰ë®Œë®¤ë®¬ë®´ë¯€ë¯ë¯„ë¯ˆë¯•ë¯¸ë¯¹ë¯¼ë¯¿ë°€ë°‹ë°Œë°ë°ë°ë°‘ë°’ë°”ë°•ë°–ë°˜ë°›ë°œë°ë°Ÿë°¤ë°¥ë°©ë°­ë°°ë°±ë°´ë°¸ë±€ë±…ë±‰ë±ë±”ë²„ë²…ë²ˆë²Œë²”ë²•ë²—ë²™ë²šë² ë²¡ë²¤ë²¨ë²³ë²¼ë²½ë³€ë³„ë³ë³‘ë³“ë³•ë³´ë³µë³¸ë³¼ë´„ë´…ë´‡ë´‰ëµ¥ë¶€ë¶ë¶„ë¶ˆë¶‰ë¶ë¶“ë¶•ë¶™ë¶ ë·°ë·´ë·¸ë·¹ë¸€ë¸Œë¸ë¸”ë¸ë¹„ë¹…ë¹ˆë¹Œë¹•ë¹—ë¹™ë¹›ë¹ ë¹¨ë¹¼ëº€ëºëº¨ë»ë»‘ë¼ˆë½€ë½‘ë½•ë¾°ë¿…ë¿Œë¿ë¿ë¿”ë¿œë¿¡ì˜ìœì ì¨ì‚ì‚”ì‚ ì‚¬ì‚­ì‚°ì‚´ì‚¶ì‚¼ì‚½ìƒìƒ‡ìƒˆìƒ‰ìƒŒìƒìƒ˜ìƒìƒ¤ìƒ¨ìƒ¬ìƒ´ìƒµìƒ·ìƒ¹ì„œì„ì„ì„ ì„¤ì„¬ì„­ì„¯ì„±ì„¸ì„¹ì„¼ì…€ì…ˆì…‰ì…‹ì…ì…”ì…˜ì…œì…¨ì…©ì…°ì†Œì†ì†ì†”ì†œì†ì†¡ì†¦ì‡„ì‡ ì‡¼ì‡½ìˆ„ìˆ˜ìˆ™ìˆœìˆ ìˆ¨ìˆ«ìˆ­ìˆ¯ìˆ±ìˆ²ì‰ì‰˜ì‰¬ì‰°ì‰´ì‰¼ì‰½ìŠˆìŠŒìŠìŠ˜ìŠ™ìŠ¤ìŠ¨ìŠ¬ìŠ­ìŠ´ìŠµìŠ·ìŠ¹ìŠ½ì‹œì‹ì‹ ì‹¤ì‹¬ì‹­ì‹±ì‹µì‹¶ì‹·ì‹¸ì‹¹ì‹¼ìŒ€ìŒìŒ“ìŒ”ìŒ•ìŒ¸ì¨ì©ì¬ì¸ì¹ì„ì¼ì˜ì™ìŸìì‘¥ì“°ì“±ì“´ì“¸ì”€ì”ì”Œì”¨ì”©ì”¬ì”¯ì”°ì”¹ì”»ì”¼ì”½ì•„ì•…ì•ˆì•‰ì•Šì•Œì•ì•“ì•”ì••ì•—ì•˜ì•™ì•›ì•ì•ì• ì•¡ì•¤ì•¨ì•°ì•±ì•´ì•µì•¼ì•½ì•¿ì–€ì–„ì–‡ì–ì–‘ì––ì–—ì–´ì–µì–¸ì–¹ì–ºì–»ì–¼ì–¿ì—„ì—…ì—†ì—‡ì—ˆì—‰ì—ì—‘ì—”ì—˜ì— ì—¡ì—£ì—¥ì—¬ì—­ì—°ì—´ì—·ì—»ì—¼ì—½ì—¿ì˜€ì˜ì˜…ì˜†ì˜ˆì˜ì˜›ì˜¤ì˜¥ì˜§ì˜¨ì˜¬ì˜®ì˜´ì˜µì˜·ì˜¹ì˜»ì˜¿ì™€ì™ì™„ì™”ì™•ì™œì™¸ì™¼ìš”ìš•ìš˜ìšœìš©ìš®ìš°ìš±ìš´ìš¸ì›€ì›ì›ƒì›…ì›Œì›ì›ì›”ì› ì›¨ì›¬ì›°ì›¹ìœ„ìœ…ìœˆìœŒìœ—ìœ ìœ¡ìœ¤ìœ¨ìœ°ìœ³ìœµìœ¼ì€ì„ì‹ìŒìì‘ì˜ì™ì«ì´ìµì¸ì»ì¼ì½ìì‚ìƒì„ì…ì‡ìˆì‰ìŠì‹ìŒìììì‘ì”ì˜ìì ì¡ì£ì¥ì¦ì¬ì­ì´ì¼ì¿ìŸìŸˆìŸì €ì ì „ì ˆì Šì ì ‘ì “ì •ì –ì œì ì  ì ¤ì ¬ì ­ì ¯ì ±ì ²ì ¶ì ¸ì ¹ì ¼ì¡Œì¡”ì¡°ì¡±ì¡´ì¡¸ì¢€ì¢ì¢…ì¢‹ì¢Œì£ ì£¼ì£½ì¤€ì¤„ì¤…ì¤Œì¤ì¤‘ì¤•ì¤˜ì¥ì¥¬ì¥°ì¥´ì¦ˆì¦‰ì¦ì¦˜ì¦™ì¦ì§€ì§ì§„ì§‡ì§ˆì§ì§‘ì§“ì§•ì§™ì§šì§›ì§œì§ì§ ì§§ì§±ì§¸ì¨°ì©¡ìª¼ìª½ìª¾ì«€ì«“ì¬ì­ˆì­‰ì¯”ì¯˜ì°Œì°ì°”ì°œì°ì°¢ì°¨ì°©ì°¬ì°®ì°°ì°¸ì°½ì°¾ì±„ì±…ì±”ì±™ì±¡ì²˜ì²™ì²œì² ì²¨ì²©ì²«ì²­ì²´ì²¼ì³ì³”ì´ˆì´‰ì´Œì´ì´˜ì´ì´¬ìµœìµ¸ì¶”ì¶•ì¶˜ì¶œì¶ì¶¤ì¶¥ì¶©ì¶«ì¶®ì¶°ì·Œì·¨ì·¸ì¸„ì¸Œì¸ ì¸¡ì¸¨ì¸µì¹˜ì¹™ì¹œì¹ ì¹¡ì¹¨ì¹«ì¹­ì¹®ì¹´ì¹¸ì¹¼ìº€ìº„ìº…ìºìº”ìº˜ìº ìº¡ìº´ì»¤ì»¨ì»¬ì»´ì»µì»·ì¼€ì¼„ì¼ˆì¼ì¼‘ì¼“ì¼šì¼œì¼ ì¼°ì½”ì½•ì½˜ì½›ì½œì½ì½¤ì½¥ì½§ì½©ì½¬ì½®ì½°ì½´ì¾Œì¿„ì¿ ì¿¤ì¿¨ì¿°ì¿³ì¿¼í€„í€˜í€´í€µí€¸í€¼í…íí˜í¬í­í°í´í¼í‚„í‚¤í‚¥í‚¨í‚¬í‚´í‚µí‚·í‚¹íƒ€íƒíƒ„íƒˆíƒíƒ‘íƒ•íƒœíƒíƒ íƒ¤íƒ¬íƒ­íƒ±íƒ¸í„°í„±í„´í„¸í…€í……í…Œí…í…í…”í…œí…í…¡í† í†¡í†¤í†¨í†°í†±í†³í†µí†¹í‡´íˆ¬íˆ­íˆ´íˆ¼í‰íŠ€íŠˆíŠœíŠ¤íŠ¬íŠ¸íŠ¹íŠ¼í‹€í‹ˆí‹í‹°í‹±í‹´í‹¸íŒ€íŒíŒ…íŒ‰íŒŒíŒíŒíŒíŒ”íŒœíŒíŒŸíŒ¡íŒ¥íŒ¨íŒ©íŒ¬íŒ¸íŒ¹íŒ»íŒ½í¼í€í„íŒí‘í˜í™íœí í¨í©í´í¸í¼í„í‰íí¬í­í°í´íºí¼íí‡í‘œí‘¸í‘¹í‘¼í’€í’ˆí’‰í’‹í’í“Œí“¨í“°í“¸í”„í”ˆí”Œí””í”™í”¼í”½í•€í•„í•Œí•í•í•‘í•˜í•™í•œí• í•£í•¤í•¨í•©í•«í•­í•®í•´í•µí•¸í•¼í–„í–‡í–ˆí–‰í–í–‘í– í–¡í–¥í—ˆí—Œí—í—˜í—™í—›í—í—¤í—¥í—¨í—¬í—´í—µí—·í—¹í—¿í˜€í˜í˜„í˜ˆí˜í˜‘í˜“í˜•í˜œí˜¸í˜¹í˜¼í™€í™ˆí™‰í™í™”í™•í™˜í™œí™¤í™©íšŒíšíšíšŸíš¡íš¨íš¬íš½í›„í›…í›Œí›”í›•í›¤í›¼íœ‘íœ˜íœœíœ©íœ´í„í‰íí‘í”í˜í™í í¡í¥í«í¬í°íˆíŒíí˜í™ï¼ƒï¼…ï¼†ï¼‡ï¼Šï¼‹ï¼ï¼šï¼œï¼ï¼ï¼¸ï¼»ï¼½ï¼¿ï½œï½ï¿£ï¿¦ï¿¨ğ›ƒ', help='character label')  # default='0123456789abcdefghijklmnopqrstuvwxyz'
        parser.add_argument('--character', type=str, default=' !"#$%&\'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\]^_`abcdefghijklmnopqrstuvwxyz{|}~Â¨ê°€ê°ê°„ê°ˆê°ê°‘ê°’ê°“ê°”ê°•ê¼´ê°–ê°—ê°™ê°œê°ê°¤ê°¬ê°­ê°±ê±€ê±ê±°ê±±ê±´ê±·ê±¸ê²€ê²ê²ƒê²…ê²‰ê¹œê²Œê²ê²ê²”ê²Ÿê² ê²¨ê²©ê²ªê²¬ê²°ê²³ê²¸ê²¹ê²¼ê²½ê³ê³„ê³ ê³¡ê³¤ê³§ê³¨ê³ªê³°ê³±ê³³ê³µê³·ê³¼ê³½ê´€ê´„ê´‘ê´œê´´êµêµ¬êµ­êµ°êµ³êµ´êµµêµ¼êµ½êµ¿ê¶ê¶ˆê¶Œê¶¤ê·€ê·„ê·œê°šê·ê· ê·¤ê·¬ê·­ê·¸ê·¹ê·¼ê·¿ê¸€ê¸ê¸ˆëº„ê¸‰ê¸‹ê¸ê¸°ê¸±ê¸´ê¸¸ê¹€ê¹ê¹…ê¹Šê¹Œê¹ê¹ê¹ê¹”ê¹ê¹¥ê¹¨êº êº¼êº¾ê»„ê»Œê»ê»‘ê»˜ê»´ê¼ˆê¼‰ê¼ê¼¬ê¼­ê¼¼ê¼½ê½‚ê½ƒê½ˆê½‰ê¾¸ê¾¹ê¿€ê¿ˆê¿ê¿ê¿”ê¿°ë€Œë„ëˆëŠëŒë“ë”ë—ëë¼ë‚€ë‚„ë‚Œë‚˜ë‚™ë‚œë‚ ë‚¨ë‚©ë‚«ë‚¬ë‚­ë‚®ë‚¯ë¦‡ë‚±ë‚´ë‚µë‚¸ë‚¼ëƒ„ëƒ…ëƒˆëƒ‰ëƒ ëƒ¥ë„ˆë„‰ë„Œë„ë„“ë„˜ë„™ë„›ë„£ë„¤ë„¥ë„¨ë„¬ë„´ë„µë„·ë…€ë…ë…„ë…ˆë…ë…•ë…›ë…¸ë…¹ë…¼ë†€ë†ˆë†ë†’ë†“ë†”ë‡Œë‡¨ëˆ„ëˆ…ëˆˆëˆŒëˆ”ëˆ•ëˆ—ëˆ™ë‰˜ë‰´ë‰¸ë‰¼ëŠ„ëŠ…ëŠëŠ”ëŠ˜ëŠ¥ëŠ¦ëŠ¬ë‹ˆë‹‰ë‹Œë‹ë‹ë‹˜ë‹™ë‹›ë‹ë‹¡ë‹¤ë‹¥ë‹¦ë‹¨ë‹«ë‹¬ë‹­ë‹®ë‹´ë‹µë‹·ë‹¹ë‹»ë‹½ë‹¿ëŒ€ëŒ„ëŒˆëŒ•ëŒœë”ë•ë˜ëœë¤ë¥ë§ë©ë®ë°ë±ë´ë¸ë€ëëŒë°ë„ë…ëˆë‹ëŒëë”ë•ë—ë™ëë ë¼ë˜ëœë ë¨ë©ë«ë‘ë‘‘ë‘”ë‘˜ë‘ ë‘¡ë‘£ë‘¥ë‘ªë’¤ë’·ë“€ë“ˆë“ë“œë“ë“ ë“£ë“¤ë“¬ë“­ë“¯ë“±ë“¸ë””ë”•ë”˜ë”œë”¤ë”¥ë”©ë”ªë”°ë”±ë”¸ë•€ë•…ë•Œë•ë•¡ë•¨ë–„ë– ë–¡ë–¤ë–¨ë–»ë–¼ë–½ë—„ë—´ë˜ë˜‘ë˜˜ë˜¥ë™¤ëšœëšëš¦ëš«ëš¯ë›°ëœ¨ëœ©ëœ¬ëœ¯ëœ°ëœ¸ëœ»ë„ë ë¤ë¼ë½ë€ë„ë¥™ë†ëŒëëë‘ë—ë˜ë™ëœê²†ë ë¨ë©ë«ë­ëµëŸ‰ëŸ¬ëŸ­ëŸ°ëŸ´ëŸ¼ëŸ½ëŸ¿ë €ë ‡ë ˆë ‰ë Œë ë ˜ë ™ë ›ë ë ¤ë ¥ë ¨ë ¬ë ´ë µë ·ë ¸ë ¹ë¡€ë¡œë¡ë¡ ë¡¤ë¡¬ë¡­ë¡¯ë¡±ë¡²ë¡¼ë¢°ë¢´ë£Œë£”ë£¡ë£¨ë£©ë£¬ë£°ë£¸ë£¹ë£»ë£½ë¤€ë¤¼ë¥˜ë¥ ë¥¨ë¥©ë¥´ë¥µë¥¸ë¥¼ë¦„ë¦…ë¦‰ë¦ë¦ë¦”ë¦¬ë¦­ë¦°ë¦´ë¦¼ë¦½ë¦¿ë§ë§„ë§‡ë§ˆë§‰ë§Œë§ë§ë§‘ë§˜ë§›ë§ë§ë§Ÿë§¡ë§¤ë§¥ë§¨ë§´ë§·ë§¹ë§»ë¨€ë¨ë¨œë¨¸ë¨¹ë¨¼ë©€ë©ë©ˆë©‹ë©ë©ë©”ë©•ë©˜ë©œë©°ë©´ë©¸ëª…ëª‡ëª”ëª¨ëª©ëª¬ëª°ëª¸ëª¹ëª»ëª½ë«¼ë¬˜ë¬´ë¬µë¬¸ë¬ºë¬»ë¬¼ë¬½ë¬¿ë­‚ë­‡ë­‰ë®Œë®¤ë®¬ë®´ë¯€ë¯ë¯„ë¯ˆë¯•ë¯¸ë¯¹ë¯¼ë¯¿ë°€ë°‹ë°Œë°ë°ë°ë°‘ë°’ë°”ë°•ë°–ë°˜ë°›ë°œë°ë°Ÿë°¤ë°¥ë°©ë°­ë°°ë°±ë°´ë°¸ë±€ë±…ë±‰ë±ë±”ë²„ë²…ë²ˆë²Œë²”ë²•ë²—ë²™ë²šë² ë²¡ë²¤ë²¨ë²³ë²¼ë²½ë³€ë³„ë³ë³‘ë³“ë³•ë³´ë³µë³¸ë³¼ë´„ë´…ë´‡ë´‰ëµ¥ë¶€ë¶ë¶„ë¶ˆë¶‰ë¶ë¶“ë¶•ë¶™ë¶ ë·°ë·´ë·¸ë·¹ë¸€ë¸Œë¸ë¸”ë¸ë¹„ë¹…ë¹ˆë¹Œë¹•ë¹—ë¹™ë¹›ë¹ ë¹¨ë¹¼ëº€ëºëº¨ë»ë»‘ë¼ˆë½€ë½‘ë½•ë¾°ë¿…ë¿Œë¿ë¿ë¿”ë¿œë¿¡ì˜ìœì ì¨ì‚ì‚”ì‚ ì‚¬ì‚­ì‚°ì‚´ì‚¶ì‚¼ì‚½ìƒìƒ‡ìƒˆìƒ‰ìƒŒìƒìƒ˜ìƒìƒ¤ìƒ¨ìƒ¬ìƒ´ìƒµìƒ·ìƒ¹ì„œì„ì„ì„ ì„¤ì„¬ì„­ì„¯ì„±ì„¸ì„¹ì„¼ì…€ì…ˆì…‰ì…‹ì…ì…”ì…˜ì…œì…¨ì…©ì…°ì†Œì†ì†ì†”ì†œì†ì†¡ì†¦ì‡„ì‡ ì‡¼ì‡½ìˆ„ìˆ˜ìˆ™ìˆœìˆ ìˆ¨ìˆ«ìˆ­ìˆ¯ìˆ±ìˆ²ì‰ì‰˜ì‰¬ì‰°ì‰´ì‰¼ì‰½ìŠˆìŠŒìŠìŠ˜ìŠ™ìŠ¤ìŠ¨ìŠ¬ìŠ­ìŠ´ìŠµìŠ·ìŠ¹ìŠ½ì‹œì‹ì‹ ì‹¤ì‹¬ì‹­ì‹±ì‹µì‹¶ì‹·ì‹¸ì‹¹ì‹¼ìŒ€ìŒìŒ“ìŒ”ìŒ•ìŒ¸ì¨ì©ì¬ì¸ì¹ì„ì¼ì˜ì™ìŸìì„°ì‘¥ì“°ì“±ì“´ì“¸ì”€ì”ì”Œì”¨ì”©ì”¬ì”¯ì”°ì”¹ì”»ì”¼ì”½ì•„ì•…ì•ˆì•‰ì•Šì•Œì•ì•“ì•”ì••ì•—ì•˜ì•™ì•›ì•ê¹ƒì•ì• ì•¡ì•¤ì•¨ì•°ì•±ì•´ì•µì•¼ì•½ì•¿ì–€ì–„ì–‡ì–ì–‘ì––ì–—ì–´ì–µì–¸ì–¹ì–ºì–»ì–¼ì–¿ì—„ì—…ì—†ì—‡ì—ˆì—‰ì—ì—‘ì—”ì—˜ì— ì—¡ì—£ì—¥ì—¬ì—­ì—°ì—´ì—·ì—»ì—¼ì—½ì—¿ì˜€ì˜ì˜…ì˜†ì˜ˆì˜ì˜›ëƒë¤„ì˜¤ì˜¥ì˜§ì˜¨ì˜¬ì˜®ì˜´ì˜µì˜·ì˜¹ì˜»ì˜¿ì™€ì™ì™„ì™”ì™•ì™œì™¸ì™¼ìš”ìš•ìš˜ìšœìš©ìš®ìš°ìš±ìš´ìš¸ì›€ì›ì›ƒì›…ì›Œì›ì›ì›”ì› ì›¨ì›¬ì›°ì›¹ìœ„ìœ…ìœˆìœŒìœ—ìœ ìœ¡ìœ¤ìœ¨ìœ°ìœ³ìœµìœ¼ì€ì„ì‹ìŒìì‘ì˜ì™ì«ì´ìµì¸ì»ì¼ì½ìì‚ìƒì„ì…ì‡ìˆì‰ì§–ìŠì‹ìŒìììì‘ì”ì˜ìì ì¡ì£ì¥ì¦ì¬ì­ì´ì¼ì¿ë§ºìŸìŸˆìŸì €ì ì „ì ˆì Šì ì ‘ì “ì •ì –ì œì ì  ì ¤ì ¬ì ­ì ¯ì ±ì ²ì ¶ì ¸ì ¹ì ¼ì¡Œì¡”ì¡°ì¡±ì¡´ì¡¸ì¢€ì¢ì¢…ì¢‹ì¢Œì£ ì£¼ì–˜ì£½ì¤€ì¤„ì¤…ì¤Œì¤ì¤‘ì¤•ì¤˜ì¥ì¥¬ì¥°ì¥´ì¦ˆì¦‰ì¦ì¦˜ì¦™ì¦ì§€ì§ì§„ëª«ì§‡ì§ˆì§ì§‘ì§“ì§•ì§™ì§šì¼ì§›ì§œì§ì§ ìª˜ì§§ì§±ì§¸ì¨°ì©¡ìª¼ìª½ìª¾ì«€ì«“ì¬ì­ˆì­‰ì¯”ì¯˜ì°Œì°ì°”ì°œì°ì°¢ì°¨ì°©ì°¬ì°®ì°°ì°¸ì°½ì°¾ì±„ì±…ì‹«ì±”ì±™ë¬¶ì±¡ì²˜ì²™ë´ì²œì² ì²¨ì²©ì²«ì²­ì²´ì²¼ì³ì³”ì´ˆì´‰ì´Œì‚¿ì´ì´˜ì´ì´¬ìµœìµ¸ì¶”ì¶•ì¶˜ì¶œì¶ì¶¤ì¶¥ì¶©ì¶«ì¶®ì¶°ì·Œì·¨ì·¸ì¸„ì¸Œì¸ ì¸¡ì¸¨ì¸µì¹˜ì¹™ì¹œì¹ ì¹¡ì¹¨ì¹«ì¹­ì¹®ì¹´ì¹¸ì¹¼ìº€ìº„ìº…íƒ“ìºìº”ìº˜ìº ìº¡ìº´ì»¤ì»¨ì»¬ì»´ì»µì»·ì¼€ì¼„ì¼ˆì¼ì¼‘ì¼“ì¼šì¼œì¼ ì¼°ì½”ì½•ì½˜ì½›ì½œì½ì½¤ì½¥ì½§ì½©ì½¬ì½®ì½°ì½´ì¾Œì¿„ì¿ ì¿¤ì¿¨ì¿°ì¿³ì¿¼í€„í€˜í€´í€µí€¸í€¼í…íí˜í¬í­í°í´í¼í‚„í‚¤í‚¥í‚¨í‚¬í‚´í‚µí‚·í‚¹íƒ€íƒíƒ„íƒˆíƒíƒ‘íƒ•íƒœíƒíƒ íƒ¤íƒ¬íƒ­íƒ±íƒ¸í„°í„±í„´í„¸í…€í……í…Œí…í…í…”í…œí…í…¡í† í†¡í†¤í†¨í†°í†±í†³í†µí†¹í‡´íˆ¬íˆ­íˆ´íˆ¼í‰íŠ€íŠˆíŠœíŠ¤íŠ¬íŠ¸íŠ¹íŠ¼í‹€í‹ˆí‹í‹°í‹±í‹´í‹¸íŒ€íŒíŒ…íŒ‰íŒŒíŒíŒíŒíŒ”íŒœíŒíŒŸíŒ¡íŒ¥íŒ¨íŒ©íŒ¬íŒ¸íŒ¹íŒ»íŒ½í¼í€í„íŒí‘í˜í™íœí í¨í©í´í¸í¼í„í‰ì£„íí¬í­í°í´íºí¼íí‡í‘œí­í‘¸í‘¹í‘¼í’€í’ˆí’‰í’‹í’í“Œí“¨í“°í“¸í”„í”ˆí”Œí””í”™í”¼í”½í•€í•„í•Œí•í•í•‘í•˜í•™í•œì³¤í• í•£í•¤í•¨í•©í•«í•­í•®í•´í•µí•¸í•¼í–„í–‡í–ˆí–‰í–í–‘í– í–¡í–¥í—ˆí—Œí—í—˜í—™í—›í—í—¤í—¥í—¨í—¬í—´í—µí—·í—¹í—¿í˜€í˜í˜„í˜ˆí˜í˜‘í˜“í˜•í˜œí›ˆí˜¸í˜¹í˜¼í™€í™ˆí™‰í™í™”í™•í™˜í™œí™¤í™©íšŒíšíšíšŸíš¡íš¨íš¬íš½í›„í›…í›Œí›”í›•í›¤í›¼íœ‘íœ˜íœœíœ©íœ´í„í‰íí‘í”í˜í™í í¡í¥í«í¬í°íˆíŒíí˜í™ï¼ƒï¼…ï¼†ï¼‡ï¼Šï¼‹ï¼ï¼šï¼œï¼ï¼ï¼¸ï¼»ï¼½ï¼¿ï½œï½ï¿£ï¿¦ï¿¨ğ›ƒê¶ë¥œëµëˆ ëì˜³ë¹šê´˜ë‚³ë‘¬ë´¤ë¹µëˆ´ë“¦ì°ê½¤ëŠ‘ì™ˆê¼°ê³¶ëµ¤', help='character label')  # default='0123456789abcdefghijklmnopqrstuvwxyz'
        parser.add_argument('--sensitive', action='store_true',
                            help='for sensitive character mode')
        parser.add_argument('--PAD', action='store_true',
                            help='whether to keep ratio then pad for image resize')
        """ Model Architecture """
        parser.add_argument('--Transformation', type=str,  # required=True,
                            default=Transformation, help='Transformation stage. None|TPS')
        parser.add_argument('--FeatureExtraction', type=str, default=FeatureExtraction,  # required=True,
                            help='FeatureExtraction stage. VGG|RCNN|ResNet')
        parser.add_argument('--SequenceModeling', type=str,  # required=True,
                            default=SequenceModeling, help='SequenceModeling stage. None|BiLSTM')
        parser.add_argument('--Prediction', type=str,  # required=True,
                            default=Prediction, help='Prediction stage. CTC|Attn')
        parser.add_argument('--num_fiducial', type=int, default=20,
                            help='number of fiducial points of TPS-STN')
        parser.add_argument('--input_channel', type=int, default=1,
                            help='the number of input channel of Feature extractor')
        parser.add_argument('--output_channel', type=int, default=512,
                            help='the number of output channel of Feature extractor')
        parser.add_argument('--hidden_size', type=int, default=256,
                            help='the size of the LSTM hidden state')

        # cho2:
        # self.opt = parser.parse_args()
        self.opt = parser.parse_args(args=[])

        """ vocab / character number configuration """
        if self.opt.sensitive:
            # same with ASTER setting (use 94 char).
            self.opt.character = string.printable[:-6]

        cudnn.benchmark = True
        cudnn.deterministic = True
        self.opt.num_gpu = torch.cuda.device_count()
        self.device = torch.device(
            'cuda' if torch.cuda.is_available() else 'cpu')
        self.model, self.AlignCollate_demo, self.converter = self.demo()

    def demo(self):
        """ model configuration """
        if 'CTC' in self.opt.Prediction:
            converter = CTCLabelConverter(self.opt.character)
        else:
            converter = AttnLabelConverter(self.opt.character)
        self.opt.num_class = len(converter.character)

        if self.opt.rgb:
            self.opt.input_channel = 3
        model = Model(self.opt)
        print('model input parameters', self.opt.imgH, self.opt.imgW, self.opt.num_fiducial, self.opt.input_channel, self.opt.output_channel,
              self.opt.hidden_size, self.opt.num_class, self.opt.batch_max_length, self.opt.Transformation, self.opt.FeatureExtraction,
              self.opt.SequenceModeling, self.opt.Prediction)
        model = torch.nn.DataParallel(model).to(self.device)

        # load model
        print('loading pretrained model from %s' % self.opt.saved_model)
        model.load_state_dict(torch.load(
            self.opt.saved_model, map_location=self.device))

        # prepare data. two demo images from https://github.com/bgshih/crnn#run-demo
        return model, AlignCollate(
            imgH=self.opt.imgH, imgW=self.opt.imgW, keep_ratio_with_pad=self.opt.PAD), converter

    # predict
    @logging_time
    def predict(self, img_name_origin, image):

        # ì—¬ê¸°ì„œ ì´ë¯¸ì§€ ë°›ìŒ
        demo_data = RawDataset(root=image,
                               opt=self.opt)  # use RawDataset
        demo_loader = torch.utils.data.DataLoader(
            demo_data, batch_size=self.opt.batch_size,
            shuffle=False,
            num_workers=int(self.opt.workers),
            collate_fn=self.AlignCollate_demo, pin_memory=True)
        self.model.eval()

        # ì—¬ê¸°ì„œ
        with torch.no_grad():
            for image_tensors, image_path_list in demo_loader:
                batch_size = image_tensors.size(0)
                image = image_tensors.to(self.device)
                # For max length prediction
                length_for_pred = torch.IntTensor(
                    [self.opt.batch_max_length] * batch_size).to(self.device)
                text_for_pred = torch.LongTensor(
                    batch_size, self.opt.batch_max_length + 1).fill_(0).to(self.device)

                # ì—¬ê¸°ê¹Œì§€ê°€ 6.6ì´ˆì •ë„
                if 'CTC' in self.opt.Prediction:
                    preds = self.model(image, text_for_pred)

                    # Select max probabilty (greedy decoding) then decode index to character
                    preds_size = torch.IntTensor([preds.size(1)] * batch_size)
                    _, preds_index = preds.max(2)
                    # preds_index = preds_index.view(-1)
                    preds_str = self.converter.decode(preds_index, preds_size)

                else:
                    preds = self.model(image, text_for_pred, is_train=False)
                    # for image_ in image:
                    #     for image__ in image_:
                    #         print(torch.sum(image__, axis=0) /
                    #               image__.shape[0])
                    #         print(torch.sum(image__, axis=1) /
                    #               image__.shape[1])
                    # select max probabilty (greedy decoding) then decode index to character
                    _, preds_index = preds.max(2)
                    preds_str = self.converter.decode(
                        preds_index, length_for_pred)

                # log = open(f'./log_demo_result.txt', 'a')
                # dashed_line = '-' * 80
                # head = f'{"image_path":25s}\t{"predicted_labels":25s}\tconfidence score'

                # print(f'{dashed_line}\n{head}\n{dashed_line}')
                # log.write(f'{dashed_line}\n{head}\n{dashed_line}\n')

                preds_prob = F.softmax(preds, dim=2)
                preds_max_prob, _ = preds_prob.max(dim=2)

                # pred_result = []
                # confidence_score_result = []

                # img_name,
                # for _, pred, pred_max_prob in zip(image_path_list, preds_str, preds_max_prob):
                #     if 'Attn' in self.opt.Prediction:
                #         pred_EOS = pred.find('[s]')
                #         # prune after "end of sentence" token ([s])
                #         pred = pred[:pred_EOS]
                #         pred_max_prob = pred_max_prob[:pred_EOS]
                #         pred_result.append(pred)

                #     # calculate confidence score (= multiply of pred_max_prob)
                #     try:
                #         confidence_score = pred_max_prob.cumprod(dim=0)[-1]
                #     except:
                #         confidence_score = 0
                #     confidence_score_result.append(confidence_score)
                #     print(
                #         f'{img_name_origin:25s}\t{pred:25s}\t{confidence_score:0.4f}')
                #     log.write(
                #         f'{img_name_origin:25s}\t{pred:25s}\t{confidence_score:0.4f}\n')
                # log.close()

                preds_str = list(map(lambda x: x[:x.find('[s]')], preds_str))

                def cs(pred_max_prob):
                    try:
                        confidence_score = pred_max_prob.cumprod(dim=0)[-1]
                    except:
                        confidence_score = 0
                    return confidence_score
                preds_max_prob = list(map(lambda x: cs(x), preds_max_prob))

                return preds_str, preds_max_prob
